ARG BASE_IMAGE_TAG=ninjacongafas/nvidia-gpu-accelerated-ai-lab:latest
FROM ${BASE_IMAGE_TAG}

USER root

RUN git clone --depth=1 https://github.com/ggerganov/llama.cpp.git /opt/llama.cpp && \
    cd /opt/llama.cpp && \
    cmake -B build \
        -DGGML_NATIVE=OFF \
        -DGGML_CUDA=ON \
        -DGGML_BACKEND_DL=ON \
        -DGGML_CPU_ALL_VARIANTS=ON \
        -DLLAMA_BUILD_TESTS=OFF \
        -DCMAKE_EXE_LINKER_FLAGS=-Wl,--allow-shlib-undefined . && \
    cmake --build build --config Release -j$(nproc) && \
    mkdir -p /opt/llama.cpp/lib && \
    find build -name "*.so" -exec cp {} /opt/llama.cpp/lib \; && \
    mkdir -p /opt/llama.cpp/full && \
    cp build/bin/* /opt/llama.cpp/full && \
    cp *.py /opt/llama.cpp/full && \
    cp -r gguf-py /opt/llama.cpp/full && \
    cp -r requirements /opt/llama.cpp/full && \
    cp requirements.txt /opt/llama.cpp/full && \
    cd /opt/llama.cpp && pip install -e . && \
    rm -rf build

USER ${USERNAME}