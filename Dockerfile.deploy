FROM ${IMAGE_NAME:-nvcr.io/nvidia/tritonserver:25.05-vllm-python-py3}
LABEL maintainer="ninja-con-gafas <el.ninja.con.gafas@gmail.com>"

ENV DEBIAN_FRONTEND=noninteractive

RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
    build-essential \
    cmake \
    git \
    curl \
    libcurl4-openssl-dev \
    libgomp1 \
    python3-pip && \
    rm -rf /var/lib/apt/lists/*

RUN git clone --depth=1 https://github.com/ggerganov/llama.cpp.git /opt/llama.cpp && \
    cd /opt/llama.cpp && \
    cmake -B build \
        -DGGML_NATIVE=OFF \
        -DGGML_CUDA=ON \
        -DGGML_BACKEND_DL=ON \
        -DGGML_CPU_ALL_VARIANTS=ON \
        -DLLAMA_BUILD_TESTS=OFF \
        -DCMAKE_EXE_LINKER_FLAGS=-Wl,--allow-shlib-undefined . && \
    cmake --build build --config Release -j$(nproc) && \
    mkdir -p /opt/llama.cpp/lib && \
    find build -name "*.so" -exec cp {} /opt/llama.cpp/lib \; && \
    mkdir -p /opt/llama.cpp/full && \
    cp build/bin/* /opt/llama.cpp/full && \
    cp *.py /opt/llama.cpp/full && \
    cp -r gguf-py /opt/llama.cpp/full && \
    cp -r requirements /opt/llama.cpp/full && \
    cp requirements.txt /opt/llama.cpp/full || true
RUN cd /opt/llama.cpp && pip install --no-cache-dir -e .
ENV PATH="/opt/llama.cpp/full:/opt/llama.cpp/build/bin:${PATH}"

ARG USERNAME=triton-server
RUN mkdir -p /workspace && \
    chown -R ${USERNAME}:${USERNAME} /workspace
USER ${USERNAME}
WORKDIR /workspace

EXPOSE 8000 8001 8002 8080